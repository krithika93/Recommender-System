{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T15:12:10.254163Z",
     "start_time": "2020-01-10T15:12:10.250630Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# data science imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T15:12:11.780127Z",
     "start_time": "2020-01-10T15:12:11.774844Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"movie recommendation\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"96g\") \\\n",
    "    .config(\"spark.driver.memory\", \"96g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.master\", \"local[12]\") \\\n",
    "    .getOrCreate()\n",
    "# get spark context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T15:12:46.747006Z",
     "start_time": "2020-01-10T15:12:14.570777Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "movies = spark.read.load('movies.csv', format='csv', header=True, inferSchema=True)\n",
    "ratings = spark.read.load('ratings.csv', format='csv', header=True, inferSchema=True)\n",
    "links = spark.read.load('links.csv', format='csv', header=True, inferSchema=True)\n",
    "tags = spark.read.load('tags.csv', format='csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T21:13:55.327934Z",
     "start_time": "2020-01-09T21:13:54.812037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieId=296, rating=5.0, userId=1),\n",
       " Row(movieId=306, rating=3.5, userId=1),\n",
       " Row(movieId=307, rating=5.0, userId=1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.mllib.recommendation import ALS,MatrixFactorizationModel,Rating\n",
    "from pyspark.sql import Row\n",
    "from collections import namedtuple\n",
    "input_path='ratings.csv'\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('recommender').getOrCreate()\n",
    "data = spark.read.format(\"csv\").option(\"header\",\"true\").load(input_path)\n",
    "parts = data.rdd\n",
    "ratingsRDD = parts.map(lambda p:Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                    \n",
    "                                    rating=float(p[2])))\n",
    "ratings= ratingsRDD.toDF()\n",
    "#print(type(ratings))\n",
    "ratings.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T08:50:15.174897Z",
     "start_time": "2020-01-10T08:50:09.212166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 296, 5.0), (1, 306, 3.5), (1, 307, 5.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_rating = sc.textFile( 'ratings.csv')\n",
    "header = movie_rating.take(1)[0]\n",
    "\n",
    "rating_data = movie_rating \\\n",
    "    .filter(lambda line: line!=header) \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))) \\\n",
    "    .cache()\n",
    "# check three rows\n",
    "rating_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T09:15:53.247276Z",
     "start_time": "2020-01-10T09:15:53.218095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[47] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, validation, test = rating_data.randomSplit([6, 2, 2], seed=99)\n",
    "# cache data\n",
    "train.cache()\n",
    "validation.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALS model selection and evaluation\n",
    "\n",
    "With the ALS model, we can use a grid search to find the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T10:50:09.414130Z",
     "start_time": "2020-01-10T09:15:54.765237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 latent factors and regularization = 0.001: validation RMSE is 0.8751816247548452\n",
      "14 latent factors and regularization = 0.01: validation RMSE is 0.8269324555975437\n",
      "14 latent factors and regularization = 0.05: validation RMSE is 0.793413480916171\n",
      "14 latent factors and regularization = 0.1: validation RMSE is 0.8059576187209331\n",
      "14 latent factors and regularization = 0.2: validation RMSE is 0.8575592844864997\n",
      "16 latent factors and regularization = 0.001: validation RMSE is 0.8872740154732262\n",
      "16 latent factors and regularization = 0.01: validation RMSE is 0.8324322208201042\n",
      "16 latent factors and regularization = 0.05: validation RMSE is 0.7929360428269683\n",
      "16 latent factors and regularization = 0.1: validation RMSE is 0.8074505448147691\n",
      "16 latent factors and regularization = 0.2: validation RMSE is 0.8587026827943351\n",
      "18 latent factors and regularization = 0.001: validation RMSE is 0.897715109950989\n",
      "18 latent factors and regularization = 0.01: validation RMSE is 0.8361875034757653\n",
      "18 latent factors and regularization = 0.05: validation RMSE is 0.7910522553694248\n",
      "18 latent factors and regularization = 0.1: validation RMSE is 0.8056276096935473\n",
      "18 latent factors and regularization = 0.2: validation RMSE is 0.8580197833462997\n",
      "20 latent factors and regularization = 0.001: validation RMSE is 0.9090440520272223\n",
      "20 latent factors and regularization = 0.01: validation RMSE is 0.8409283111921583\n",
      "20 latent factors and regularization = 0.05: validation RMSE is 0.7913233184030777\n",
      "20 latent factors and regularization = 0.1: validation RMSE is 0.8056121569943138\n",
      "20 latent factors and regularization = 0.2: validation RMSE is 0.8573913032887116\n",
      "\n",
      "The best model has 18 latent factors and regularization = 0.05\n",
      "Total Runtime: 5654.64 seconds\n"
     ]
    }
   ],
   "source": [
    "def train_ALS(train_data, validation_data, num_iters, reg_param, ranks):\n",
    "    \"\"\"\n",
    "    Grid Search Function to select the best model based on RMSE of hold-out data\n",
    "    \"\"\"\n",
    "    # initial\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    for rank in ranks:\n",
    "        for reg in reg_param:\n",
    "            # train ALS model\n",
    "            model = ALS.train(\n",
    "                ratings=train_data,    # (userID, productID, rating) tuple\n",
    "                iterations=num_iters,\n",
    "                rank=rank,\n",
    "                lambda_=reg,           # regularization param\n",
    "                seed=99)\n",
    "            # make prediction\n",
    "            valid_data = validation_data.map(lambda p: (p[0], p[1]))\n",
    "            predictions = model.predictAll(valid_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "            # get the rating result\n",
    "            ratesAndPreds = validation_data.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "            # get the RMSE\n",
    "            MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "            error = math.sqrt(MSE)\n",
    "            print('{} latent factors and regularization = {}: validation RMSE is {}'.format(rank, reg, error))\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors and regularization = {}'.format(best_rank, best_regularization))\n",
    "    return best_model\n",
    "\n",
    "\n",
    "num_iterations = 10\n",
    "ranks = [14, 16, 18, 20]\n",
    "reg_params = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "\n",
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model = train_ALS(train, validation, num_iterations, reg_params, ranks)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the tune_ALS function in two parts and in the first part I considered the latent factors like [8,10,12] with respective regularization terms and got the following results and yet found that best model has 18 latent factors and reg term of 0.05, as it has the lowest RMSE \n",
    "\n",
    "These are the results of the latent factors like [8,10,12]:\n",
    "\n",
    "\n",
    "8 latent factors and regularization = 0.001: validation RMSE is 0.8410059570570684\n",
    "8 latent factors and regularization = 0.01: validation RMSE is 0.8152786348189429\n",
    "8 latent factors and regularization = 0.05: validation RMSE is 0.8028483085096276\n",
    "8 latent factors and regularization = 0.1: validation RMSE is 0.8097231653772525\n",
    "8 latent factors and regularization = 0.2: validation RMSE is 0.8570296996940661\n",
    "10 latent factors and regularization = 0.001: validation RMSE is 0.850314971518225\n",
    "10 latent factors and regularization = 0.01: validation RMSE is 0.818705066471433\n",
    "10 latent factors and regularization = 0.05: validation RMSE is 0.7983040640059628\n",
    "10 latent factors and regularization = 0.1: validation RMSE is 0.8078940255789443\n",
    "10 latent factors and regularization = 0.2: validation RMSE is 0.8575775776051373\n",
    "12 latent factors and regularization = 0.001: validation RMSE is 0.859409779870995\n",
    "12 latent factors and regularization = 0.01: validation RMSE is 0.8214563848677076\n",
    "12 latent factors and regularization = 0.05: validation RMSE is 0.7938363743722037\n",
    "12 latent factors and regularization = 0.1: validation RMSE is 0.804776318537385\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T16:24:58.773897Z",
     "start_time": "2020-01-10T16:20:44.278853Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 latent factors and regularization = 0.2: validation RMSE is 0.8567586006292939\n"
     ]
    }
   ],
   "source": [
    "# get ALS model\n",
    "maxIter = 10\n",
    "setRank = 12\n",
    "reg = 0.2\n",
    "model = ALS.train(\n",
    "    ratings=train,    # (userID, productID, rating) tuple\n",
    "    iterations=maxIter,\n",
    "    rank=setRank,\n",
    "    lambda_=reg,           # regularization param\n",
    "    seed=99)\n",
    "# make prediction\n",
    "valid_data = validation.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(valid_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "# get the rating result\n",
    "ratesAndPreds = validation.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "# get the RMSE\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "error = math.sqrt(MSE)\n",
    "print('{} latent factors and regularization = {}: validation RMSE is {}'.format(setRank, reg, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T17:18:35.616825Z",
     "start_time": "2020-01-10T17:18:35.602957Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_movieId(df_movies, fav_movie_list):\n",
    "    \"\"\"\n",
    "    return all movieId(s) of user's favorite movies\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_movies: spark Dataframe, movies data\n",
    "    \n",
    "    fav_movie_list: list, user's list of favorite movies\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    movieId_list: list of movieId(s)\n",
    "    \"\"\"\n",
    "    movieId_list = []\n",
    "    for movie in fav_movie_list:\n",
    "        movieIds = df_movies \\\n",
    "            .filter(movies.title.like('%{}%'.format(movie))) \\\n",
    "            .select('movieId') \\\n",
    "            .rdd \\\n",
    "            .map(lambda r: r[0]) \\\n",
    "            .collect()\n",
    "        movieId_list.extend(movieIds)\n",
    "    return list(set(movieId_list))\n",
    "\n",
    "\n",
    "def add_new_user_to_data(train_data, movieId_list, spark_context):\n",
    "    \"\"\"\n",
    "    add new rows with new user, user's movie and ratings to\n",
    "    existing train data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: spark RDD, ratings data\n",
    "    \n",
    "    movieId_list: list, list of movieId(s)\n",
    "\n",
    "    spark_context: Spark Context object\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    new train data with the new user's rows\n",
    "    \"\"\"\n",
    "    # get new user id\n",
    "    new_id = train_data.map(lambda r: r[0]).max() + 1\n",
    "    # get max rating\n",
    "    max_rating = train_data.map(lambda r: r[2]).max()\n",
    "    # create new user rdd\n",
    "    user_rows = [(new_id, movieId, max_rating) for movieId in movieId_list]\n",
    "    new_rdd = spark_context.parallelize(user_rows)\n",
    "    # return new train data\n",
    "    return train_data.union(new_rdd)\n",
    "\n",
    "\n",
    "def get_inference_data(train_data, df_movies, movieId_list):\n",
    "    \"\"\"\n",
    "    return a rdd with the userid and all movies (except ones in movieId_list)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: spark RDD, ratings data\n",
    "\n",
    "    df_movies: spark Dataframe, movies data\n",
    "    \n",
    "    movieId_list: list, list of movieId(s)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    inference data: Spark RDD\n",
    "    \"\"\"\n",
    "    # get new user id\n",
    "    new_id = train_data.map(lambda r: r[0]).max() + 1\n",
    "    # return inference rdd\n",
    "    return df_movies.rdd \\\n",
    "        .map(lambda r: r[0]) \\\n",
    "        .distinct() \\\n",
    "        .filter(lambda x: x not in movieId_list) \\\n",
    "        .map(lambda x: (new_id, x))\n",
    "\n",
    "\n",
    "def make_recommendation(best_model_params, ratings_data, df_movies, \n",
    "                        fav_movie_list, n_recommendations, spark_context):\n",
    "    \"\"\"\n",
    "    return top n movie recommendation based on user's input list of favorite movies\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    best_model_params: dict, {'iterations': iter, 'rank': rank, 'lambda_': reg}\n",
    "\n",
    "    ratings_data: spark RDD, ratings data\n",
    "\n",
    "    df_movies: spark Dataframe, movies data\n",
    "\n",
    "    fav_movie_list: list, user's list of favorite movies\n",
    "\n",
    "    n_recommendations: int, top n recommendations\n",
    "\n",
    "    spark_context: Spark Context object\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    list of top n movie recommendations\n",
    "    \"\"\"\n",
    "    # modify train data by adding new user's rows\n",
    "    movieId_list = get_movieId(df_movies, fav_movie_list)\n",
    "    train_data = add_new_user_to_data(ratings_data, movieId_list, spark_context)\n",
    "    \n",
    "    # train best ALS\n",
    "    model = ALS.train(\n",
    "        ratings=train_data,\n",
    "        iterations=best_model_params.get('iterations', None),\n",
    "        rank=best_model_params.get('rank', None),\n",
    "        lambda_=best_model_params.get('lambda_', None),\n",
    "        seed=99)\n",
    "    \n",
    "    # get inference rdd\n",
    "    inference_rdd = get_inference_data(ratings_data, df_movies, movieId_list)\n",
    "    \n",
    "    # inference\n",
    "    predictions = model.predictAll(inference_rdd).map(lambda r: (r[1], r[2]))\n",
    "    \n",
    "    # get top n movieId\n",
    "    topn_rows = predictions.sortBy(lambda r: r[1], ascending=False).take(n_recommendations)\n",
    "    topn_ids = [r[0] for r in topn_rows]\n",
    "    \n",
    "    # return movie titles\n",
    "    return df_movies.filter(movies.movieId.isin(topn_ids)) \\\n",
    "                    .select('title') \\\n",
    "                    .rdd \\\n",
    "                    .map(lambda r: r[0]) \\\n",
    "                    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T17:23:33.942107Z",
     "start_time": "2020-01-10T17:19:17.793568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for Iron Man:\n",
      "1: Jack-O (1995)\n",
      "2: Dragon Fist (Long quan) (1979)\n",
      "3: Enemies of Reason, The (2007)\n",
      "4: Secrets Of State (2008)\n",
      "5: Levitated Mass (2013)\n",
      "6: Being in the World (2009)\n",
      "7: Dara O'Briain Crowd Tickler (2015)\n",
      "8: Midnight Diner (2014)\n",
      "9: A Question of Faith (2017)\n",
      "10: Towed in a Hole (1932)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# my favorite movies\n",
    "my_favorite_movies = ['Iron Man']\n",
    "\n",
    "# get recommends\n",
    "recommends = make_recommendation(\n",
    "    best_model_params={'iterations': 10, 'rank': 20, 'lambda_': 0.05}, \n",
    "    ratings_data=rating_data, \n",
    "    df_movies=movies, \n",
    "    fav_movie_list=my_favorite_movies, \n",
    "    n_recommendations=10, \n",
    "    spark_context=sc)\n",
    "\n",
    "print('Recommendations for {}:'.format(my_favorite_movies[0]))\n",
    "for i, title in enumerate(recommends):\n",
    "    print('{0}: {1}'.format(i+1, title))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
